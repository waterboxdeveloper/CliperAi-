version: '3.8'

services:
  cliper:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cliper_app
    environment:
      # IMPORTANT: Set these environment variables in your host system
      # or create a .env file in the project root
      # Docker Compose will automatically load variables from .env
      GOOGLE_API_KEY: ${GOOGLE_API_KEY}

      # Optional: Configure Whisper model size
      WHISPER_MODEL: ${WHISPER_MODEL:-base}

      # Optional: Set log level for debugging
      LOG_LEVEL: ${LOG_LEVEL:-INFO}

      # Optional: Other settings from .env
      WHISPER_LANGUAGE: ${WHISPER_LANGUAGE:-auto}
      MIN_CLIP_DURATION: ${MIN_CLIP_DURATION:-30}
      MAX_CLIP_DURATION: ${MAX_CLIP_DURATION:-90}
      MAX_CLIPS: ${MAX_CLIPS:-100}
      GEMINI_MODEL: ${GEMINI_MODEL:-gemini-2.0-flash-exp}
      GEMINI_TEMPERATURE: ${GEMINI_TEMPERATURE:-0.8}
      MAX_COPY_GENERATION_ATTEMPTS: ${MAX_COPY_GENERATION_ATTEMPTS:-3}
      OUTPUT_DIR: ${OUTPUT_DIR:-./output}
      DOWNLOADS_DIR: ${DOWNLOADS_DIR:-./downloads}
      TEMP_DIR: ${TEMP_DIR:-./temp}

    volumes:
      # Mount the current project directory into the container
      # This allows CLIPER to access your input videos and save outputs
      - .:/app

      # Mount a dedicated volume for WhisperX models to persist them
      # This prevents re-downloading models every time the container starts
      - cliper_whisper_models:/root/.cache/whisperx

    # If you need GPU support for WhisperX (highly recommended for performance)
    # uncomment the following lines and ensure your Docker setup supports NVIDIA GPUs
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

    stdin_open: true # Keep stdin open for interactive CLI
    tty: true        # Allocate a pseudo-TTY for interactive CLI

volumes:
  cliper_whisper_models:
    driver: local
